{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nfrom tqdm import tqdm\nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########################\n# Augmentation methods\n#########################\ndef noise(data):\n    \"\"\"\n    Adding White Noise.\n    \"\"\"\n    # you can take any distribution from https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html\n    noise_amp = 0.05*np.random.uniform()*np.amax(data)   # more noise reduce the value to 0.5\n    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])\n    return data\n    \ndef shift(data):\n    \"\"\"\n    Random Shifting.\n    \"\"\"\n    s_range = int(np.random.uniform(low=-5, high = 5)*1000)  #default at 500\n    return np.roll(data, s_range)\n    \ndef stretch(data, rate=0.8):\n    \"\"\"\n    Streching the Sound. Note that this expands the dataset slightly\n    \"\"\"\n    data = librosa.effects.time_stretch(data, rate)\n    return data\n    \ndef pitch(data, sample_rate):\n    \"\"\"\n    Pitch Tuning.\n    \"\"\"\n    bins_per_octave = 12\n    pitch_pm = 2\n    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n    data = librosa.effects.pitch_shift(data.astype('float64'), \n                                      sample_rate, n_steps=pitch_change, \n                                      bins_per_octave=bins_per_octave)\n    return data\n    \ndef dyn_change(data):\n    \"\"\"\n    Random Value Change.\n    \"\"\"\n    dyn_change = np.random.uniform(low=-0.5 ,high=7)  # default low = 1.5, high = 3\n    return (data * dyn_change)\n    \ndef speedNpitch(data):\n    \"\"\"\n    peed and Pitch Tuning.\n    \"\"\"\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ref2 = pd.read_csv(\"../input/final3/FINAL3.csv\")\nref = ref2[['labels', 'source', 'path']].copy()\nref.head()\nref.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note this takes a couple of minutes (~16 mins) as we're iterating over 4 datasets, and with augmentation  \ndf = pd.DataFrame(columns=['feature'])\ndf_noise = pd.DataFrame(columns=['feature'])\ndf_speedpitch = pd.DataFrame(columns=['feature'])\ncnt = 0\n\n# loop feature extraction over the entire dataset\nfor i in tqdm(ref.path):\n    \n    # first load the audio \n    X, sample_rate = librosa.load(i\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n\n    # take mfcc and mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=np.array(sample_rate), \n                                        n_mfcc=100).T,\n                    axis=0)\n    \n    df.loc[cnt] = [mfccs]   \n\n    # random shifting (omit for now)\n    # Stretch\n    # pitch (omit for now)\n    # dyn change\n    \n    # noise \n    aug = noise(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=100).T,    \n                  axis=0)\n    df_noise.loc[cnt] = [aug]\n\n    # speed pitch\n    aug = speedNpitch(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=100).T,    \n                  axis=0)\n    df_speedpitch.loc[cnt] = [aug]   \n\n    cnt += 1\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine \ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf_noise = pd.concat([ref,pd.DataFrame(df_noise['feature'].values.tolist())],axis=1)\ndf_speedpitch = pd.concat([ref,pd.DataFrame(df_speedpitch['feature'].values.tolist())],axis=1)\nprint(df.shape,df_noise.shape,df_speedpitch.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,df_noise,df_speedpitch],axis=0,sort=False)\ndf=df.fillna(0)\ndel df_noise, df_speedpitch\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.2\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[1500:1520]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\n\n# Check the dataset now \nX_train[150:160]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean)\nfrom numpy import savetxt\nsavetxt('mean.csv', mean, delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(std)\nsavetxt('std.csv', std, delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape\n# X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(100,1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(7)) # Target class number\nmodel.add(Activation('softmax'))\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='Adagrad',metrics=['accuracy'])\n# model_history=model.fit(X_train, y_train, batch_size=16, epochs=130, validation_data=(X_test, y_test),verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"Jan7.json\", \"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"Jan7.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"Jan7.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(X_train[2].shape)\n# y=X_train[786].reshape(1,100,1)\n# print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(model.predict(y))\n# print(y_train[786])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('/kaggle/input/newmodel/Jan7.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.metrics_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading json and model architecture \njson_file = open('/kaggle/working/model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"../input/693105012020/Emotion_Model_aug 6931.h5\")\nprint(\"Loaded model from disk\")\n\n# the optimiser\nloaded_model.compile(loss='categorical_crossentropy', optimizer='Adagrad',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Lets transform the dataset so we can apply the predictions\nX, sample_rate = librosa.load('/kaggle/input/happy-audio/Liza-happy-v3.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets transform the dataset so we can apply the predictions\nX, sample_rate = librosa.load('input/CREMA-D/AudioWAV/1001_IEO_HAP_MD.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=np.array(sample_rate), n_mfcc=100).T,axis=0)\n\nmean = np.mean(mfccs, axis=0)\nstd = np.std(mfccs, axis=0)\n\nmfccs = (mfccs - mean)/std\n\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply predictions\nnewdf= np.expand_dims(newdf, axis=2)\nnewpred = model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[1:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(X_train[10:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train[10:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}